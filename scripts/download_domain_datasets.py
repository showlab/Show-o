import os
import json
import requests
from pathlib import Path
from tqdm import tqdm
from PIL import Image


def download_file(url, output_path, description="Downloading"):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    response = requests.get(url, stream=True)
    total_size = int(response.headers.get('content-length', 0))
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'wb') as f, tqdm(
        desc=description,
        total=total_size,
        unit='B',
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                bar.update(len(chunk))


def download_vqav2(output_dir="./data/vqav2"):
    print("=" * 60)
    print("VQAv2 Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ VQAv2 —á–µ—Ä–µ–∑ Hugging Face...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train split (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö)
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ train split (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
        dataset = load_dataset("HuggingFaceM4/VQAv2", split="train")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
        max_samples = 50000  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 50k –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ ~443k
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ {max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É–±–µ—Ä–∏—Ç–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ)")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}  # –ß—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–∞–π—Å–∏–Ω–≥ –≤–º–µ—Å—Ç–æ take –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        dataset_subset = dataset.select(range(min(max_samples, len(dataset))))
        
        for idx, item in enumerate(tqdm(dataset_subset, desc="Processing", total=min(max_samples, len(dataset)))):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑
            image = item['image']
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if 'image_id' in item:
                image_key = f"vqav2_{item['image_id']}"
            else:
                image_key = f"vqav2_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            answers = item.get('answers', [])
            if isinstance(answers, list):
                # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
                if len(answers) > 0:
                    if isinstance(answers[0], dict):
                        # –§–æ—Ä–º–∞—Ç: [{"answer": "yes", "answer_confidence": "yes"}, ...]
                        answer_texts = [a.get('answer', '') for a in answers if a.get('answer')]
                        answer = answer_texts[0] if answer_texts else ''
                    else:
                        answer_texts = answers
                        answer = answer_texts[0] if answer_texts else ''
                else:
                    answer = ''
            else:
                answer = str(answers) if answers else ''
                answer_texts = [answer]
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts[:5] if isinstance(answer_texts, list) else [answer],  # –¢–æ–ø-5 –æ—Ç–≤–µ—Ç–æ–≤
                "answer": answer
            }
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ VQAv2 –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_textvqa(output_dir="./data/textvqa"):
    print("=" * 60)
    print("TextVQA Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ TextVQA —á–µ—Ä–µ–∑ Hugging Face...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train split
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ train split...")
        dataset = load_dataset("textvqa", split="train")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        max_samples = 30000  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 30k –ø—Ä–∏–º–µ—Ä–æ–≤
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ {max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ª–∞–π—Å–∏–Ω–≥ –≤–º–µ—Å—Ç–æ take –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        dataset_subset = dataset.select(range(min(max_samples, len(dataset))))
        
        for idx, item in enumerate(tqdm(dataset_subset, desc="Processing", total=min(max_samples, len(dataset)))):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = item['image']
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            if 'image_id' in item:
                image_key = f"textvqa_{item['image_id']}"
            else:
                image_key = f"textvqa_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            answers = item.get('answers', [])
            if isinstance(answers, list):
                answer_texts = answers[:5]  # –¢–æ–ø-5
                answer = answers[0] if len(answers) > 0 else ''
            else:
                answer = str(answers) if answers else ''
                answer_texts = [answer]
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts,
                "answer": answer
            }
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ TextVQA –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_clevr(output_dir="./data/clevr"):
    print("=" * 60)
    print("CLEVR Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        import zipfile
        from datasets import load_dataset
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ Hugging Face —Å–Ω–∞—á–∞–ª–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ–¥ –¥—Ä—É–≥–∏–º –∏–º–µ–Ω–µ–º)
        print("üì¶ –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CLEVR —á–µ—Ä–µ–∑ Hugging Face...")
        dataset_names = ["allenai/clevr-dataset", "yujiali/clevr-dataset-gen"]
        
        dataset = None
        for name in dataset_names:
            try:
                print(f"   –ü—Ä–æ–±—É–µ–º {name}...")
                dataset = load_dataset(name, split="train")
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ {name}")
                break
            except Exception:
                continue
        
        if dataset is None:
            # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —á–µ—Ä–µ–∑ Hugging Face, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –∑–∞–≥—Ä—É–∑–∫—É
            print("   ‚ö†Ô∏è  Hugging Face –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –∑–∞–≥—Ä—É–∑–∫—É")
            print("\n   –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ CLEVR –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ:")
            print("   1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://cs.stanford.edu/people/jcjohns/clevr/")
            print("   2. –°–∫–∞—á–∞—Ç—å CLEVR_v1.0.zip")
            print("   3. –†–∞—Å–ø–∞–∫–æ–≤–∞—Ç—å –≤", output_dir)
            print("   4. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å:")
            print("      clevr/")
            print("        images/")
            print("          train/")
            print("            CLEVR_train_*.png")
            print("        questions/")
            print("          CLEVR_train_questions.json")
            print("\n   –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥—Ä—É–≥–æ–π –¥–∞—Ç–∞—Å–µ—Ç (--vizwiz)")
            return False
        
        print(f"   –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(dataset)}")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        
        for idx, item in enumerate(tqdm(dataset, desc="Processing")):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = item.get('image')
            if image is None:
                continue
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            if 'image_filename' in item:
                image_key = item['image_filename'].replace('.png', '').replace('.jpg', '')
            elif 'image_id' in item:
                image_key = f"clevr_{item['image_id']}"
            else:
                image_key = f"clevr_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            answer = item.get('answer', '')
            if not answer:
                answers = item.get('answers', [])
                if isinstance(answers, list) and len(answers) > 0:
                    answer = answers[0] if isinstance(answers[0], str) else str(answers[0])
                else:
                    answer = str(answers) if answers else ''
            
            answer_texts = [answer] if answer else []
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts,
                "answer": answer
            }
            
            if 'question_id' in item:
                data_item['question_id'] = item['question_id']
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ CLEVR –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_vizwiz(output_dir="./data/vizwiz"):
    print("=" * 60)
    print("VizWiz Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ VizWiz —á–µ—Ä–µ–∑ Hugging Face...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train split
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ train split (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
        dataset = load_dataset("vizwiz", split="train")
        
        total_available = len(dataset)
        print(f"   –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_available}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        
        for idx, item in enumerate(tqdm(dataset, desc="Processing")):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = item.get('image')
            if image is None:
                print(f"   ‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω –ø—Ä–∏–º–µ—Ä {idx}: –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                continue
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á
            if 'image_id' in item:
                image_key = f"vizwiz_{item['image_id']}"
            elif 'imageId' in item:
                image_key = f"vizwiz_{item['imageId']}"
            else:
                image_key = f"vizwiz_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            # VizWiz –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –ø–æ–ª–µ "answer" –∏–ª–∏ "answers"
            answer = item.get('answer', '')
            if not answer:
                answers = item.get('answers', [])
                if isinstance(answers, list) and len(answers) > 0:
                    if isinstance(answers[0], dict):
                        # –ú–æ–∂–µ—Ç –±—ã—Ç—å —Ñ–æ—Ä–º–∞—Ç [{"answer": "...", ...}, ...]
                        answer_texts = [a.get('answer', '') for a in answers if a.get('answer')]
                        answer = answer_texts[0] if answer_texts else ''
                    else:
                        answer_texts = [str(a) for a in answers[:5]]
                        answer = answer_texts[0] if answer_texts else ''
                else:
                    answer = str(answers) if answers else ''
                    answer_texts = [answer] if answer else []
            else:
                answer_texts = [answer]
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts[:5] if isinstance(answer_texts, list) else [answer],
                "answer": answer
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'question_id' in item:
                data_item['question_id'] = item['question_id']
            elif 'questionId' in item:
                data_item['question_id'] = item['questionId']
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ VizWiz –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_docvqa(output_dir="./data/docvqa"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç DocVQA –¥–∞—Ç–∞—Å–µ—Ç
    
    DocVQA –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —á–µ—Ä–µ–∑:
    1. Hugging Face: https://huggingface.co/datasets/ashraq/docvqa
    2. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: https://rrc.cvc.uab.es/?ch=17
    """
    print("=" * 60)
    print("DocVQA Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ß–µ—Ä–µ–∑ Hugging Face (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ DocVQA —á–µ—Ä–µ–∑ Hugging Face...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º train split
        dataset = load_dataset("ashraq/docvqa", split="train")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        for idx, item in enumerate(tqdm(dataset, desc="Processing")):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = item['image']
            image_filename = f"docvqa_{idx:06d}.png"
            image_path = images_dir / image_filename
            image.save(image_path)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç –Ω–∞—à –¥–∞—Ç–∞—Å–µ—Ç
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": item.get('answers', []),
                "answer": item.get('answers', [''])[0] if item.get('answers') else ''
            }
            
            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            data_item.update({
                "questionId": item.get('questionId', ''),
                "ucsf_document_id": item.get('ucsf_document_id', ''),
                "ucsf_document_page_no": item.get('ucsf_document_page_no', 0),
            })
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ DocVQA –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        print("\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç:")
        print("1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://rrc.cvc.uab.es/?ch=17")
        print("2. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç")
        print("3. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤ –≤", output_dir)
        print("4. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–µ–¥—É—é—â–∞—è:")
        print("   docvqa/")
        print("     train.json")
        print("     images/")
        print("       *.png")
        return False


def download_kvasir_vqa(output_dir="./data/kvasir"):
    print("=" * 60)
    print("Kvasir-VQA Dataset Download")
    print("=" * 60)
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    print("üì¶ Kvasir-VQA –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ GitHub")
    print("   –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: https://github.com/simula/kvasir-vqa")
    print("\n–î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ Hugging Face –∏–ª–∏ –ø—Ä—è–º–æ–π URL...")
    
    # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ Hugging Face
    try:
        from datasets import load_dataset
        print("üì¶ –ü–æ–∏—Å–∫ Kvasir-VQA –Ω–∞ Hugging Face...")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π
        dataset_names = [
            "kvasir-vqa",
            "simula/kvasir-vqa",
            "medical-vqa/kvasir",
        ]
        
        dataset = None
        for name in dataset_names:
            try:
                dataset = load_dataset(name, split="train")
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {name}")
                break
            except:
                continue
        
        if dataset is None:
            raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ Hugging Face")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        for idx, item in enumerate(tqdm(dataset, desc="Processing")):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if 'image' in item:
                image = item['image']
                image_filename = f"kvasir_{idx:06d}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                
                image_path_str = image_filename
            elif 'image_path' in item:
                image_path_str = item['image_path']
            else:
                continue
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            data_item = {
                "image": image_path_str,
                "question": item.get('question', item.get('Question', '')),
                "answers": item.get('answers', item.get('Answers', [])),
                "answer": item.get('answer', item.get('Answer', ''))
            }
            
            # –ï—Å–ª–∏ answers - —Å–ø–∏—Å–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –∫–∞–∫ answer
            if 'answers' in item and isinstance(item['answers'], list) and len(item['answers']) > 0:
                data_item['answer'] = item['answers'][0]
            elif 'Answers' in item and isinstance(item['Answers'], list) and len(item['Answers']) > 0:
                data_item['answer'] = item['Answers'][0]
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Kvasir-VQA –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        print("\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç - —Ä—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:")
        print("1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://github.com/simula/kvasir-vqa")
        print("2. –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ README")
        print("3. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤", output_dir)
        print("4. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–µ–¥—É—é—â–∞—è:")
        print("   kvasir/")
        print("     train.json")
        print("     images/")
        print("       *.jpg –∏–ª–∏ *.png")
        print("\n–ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞ train.json:")
        print('  [')
        print('    {')
        print('      "image": "image_001.jpg",')
        print('      "question": "What is visible in the image?",')
        print('      "answers": ["answer1", "answer2"],')
        print('      "answer": "answer1"')
        print('    },')
        print('    ...')
        print('  ]')
        return False


def create_sample_json(output_dir, dataset_name, num_samples=10):
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä JSON
    sample_data = []
    for i in range(num_samples):
        sample_data.append({
            "image": f"{dataset_name}_sample_{i:03d}.png",
            "question": f"Sample question {i}?",
            "answers": [f"Sample answer {i} A", f"Sample answer {i} B"],
            "answer": f"Sample answer {i} A"
        })
        
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        img_path = images_dir / f"{dataset_name}_sample_{i:03d}.png"
        img = Image.new('RGB', (256, 256), color=(128, 128, 128))
        img.save(img_path)
    
    json_path = output_dir / "train.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω –ø—Ä–∏–º–µ—Ä JSON: {json_path}")
    print(f"   –°–æ–∑–¥–∞–Ω–æ {num_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø—É—Å—Ç—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–º–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤')
    parser.add_argument('--vqav2', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å VQAv2 (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)')
    parser.add_argument('--textvqa', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å TextVQA (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)')
    parser.add_argument('--both', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±–∞ –ª–µ–≥–∫–æ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ (VQAv2 + TextVQA)')
    parser.add_argument('--clevr', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å CLEVR (–Ω–µ–±–æ–ª—å—à–æ–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç, ~100-200 –ú–ë)')
    parser.add_argument('--vizwiz', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å VizWiz (–Ω–µ–±–æ–ª—å—à–æ–π –¥–∞—Ç–∞—Å–µ—Ç, ~1-2 –ì–ë)')
    parser.add_argument('--small-datasets', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–±–∞ –Ω–µ–±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞ (CLEVR + VizWiz)')
    parser.add_argument('--docvqa', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å DocVQA')
    parser.add_argument('--kvasir', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å Kvasir-VQA')
    parser.add_argument('--create-samples', action='store_true', 
                       help='–°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã JSON —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--output-dir', type=str, default="./data",
                       help='–ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: ./data)')
    
    args = parser.parse_args()
    
    if args.create_samples:
        print("üìù –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ JSON —Ñ–∞–π–ª–æ–≤...")
        create_sample_json(f"{args.output_dir}/vqav2", "vqav2", 10)
        create_sample_json(f"{args.output_dir}/textvqa", "textvqa", 10)
        print("\n‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Å–æ–∑–¥–∞–Ω—ã. –í—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.")
        print("   –ü–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∑–∞–º–µ–Ω–∏—Ç–µ train.json –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
        return
    
    success_count = 0
    if args.both or args.vqav2:
        if download_vqav2(f"{args.output_dir}/vqav2"):
            success_count += 1
    
    if args.both or args.textvqa:
        if download_textvqa(f"{args.output_dir}/textvqa"):
            success_count += 1
    
    if args.small_datasets or args.clevr:
        if download_clevr(f"{args.output_dir}/clevr"):
            success_count += 1
    
    if args.small_datasets or args.vizwiz:
        if download_vizwiz(f"{args.output_dir}/vizwiz"):
            success_count += 1
    
    if args.docvqa:
        if download_docvqa(f"{args.output_dir}/docvqa"):
            success_count += 1
    
    if args.kvasir:
        if download_kvasir_vqa(f"{args.output_dir}/kvasir"):
            success_count += 1
    
    if success_count == 0:
        print("\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç—ã.")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --create-samples –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ JSON —Ñ–∞–π–ª–æ–≤.")
        print("–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –≤—Ä—É—á–Ω—É—é —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤—ã—à–µ.")


if __name__ == "__main__":
    main()

