wandb:
  entity: null
  resume: 'auto'

experiment:
    project: "showo2-7b-stage-1-release"
    name: "showo2-7b-stage-1-b"
    output_dir: "showo2-7b-stage-1-b"
    output_dataloader_state_dir: "128gpus_dataloader_states"
    max_train_examples_t2i: 10000000
    max_train_examples_mmu: 66000000
    save_every: 10000
    generate_every: 1000
    log_every: 50
    log_grad_norm_every: 500
    resume_from_checkpoint: 'latest'

model:
    vae_model:
        type: "wan21"
        pretrained_model_path: "Wan2.1_VAE.pth"

    showo:
        model_name: "Showo2"
        load_from_showo: False
        pretrained_model_path: ""
        llm_model_path: "Qwen/Qwen2.5-7B-Instruct"
        llm_vocab_size: null # will be updated when setting the tokenizer in other code files
        hidden_size: 3584
        image_latent_dim: 16
        image_latent_height: 27
        image_latent_width: 27
        hq_image_latent_height: 64
        hq_image_latent_width: 64
        mixed_modal_latent_height: 27
        mixed_modal_latent_width: 27
        patch_size: 2
        num_diffusion_layers: 10
        clip_latent_dim: 1152
        add_qk_norm: True
        add_time_embeds: True
        frozen_params: ['image_embedder_und', 'und_trans', 'showo', 'position_embedding']
        params_not_load: null

    clip:
        pretrained_model_path: "google/siglip-so400m-patch14-384"

    gradient_checkpointing: True

dataset:
    samp_probs: null
    accumulation: 1
    mixed_loader_mode: "concat_max_size_cycle"
    params:
        train_t2i_shards_path_or_url: "path/to/your.jsonl"
        train_mmu_shards_path_or_url: "path/to/your.jsonl"
        is_clip_encoder: False
        default_system_prompt: ""
        add_caption_prompt: True
        validation_prompts_file: "prompts/t2i_prompts.txt"
        shuffle_buffer_size: 1000
        num_workers: 6
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 1024
        max_hq_seq_length: 4352
        max_mixed_modal_seq_length: 5120
        resolution: 432
        hq_resolution: 1024
        mixed_modal_resolution: 432
        num_t2i_image_tokens: 729
        num_mmu_image_tokens: 729
        num_hq_image_tokens: 4096
        num_mixed_modal_tokens: 729
        num_video_tokens: 3645
        latent_height: ${model.showo.image_latent_height}
        latent_width: ${model.showo.image_latent_width}
        hq_latent_height: ${model.showo.hq_image_latent_height}
        hq_latent_width: ${model.showo.hq_image_latent_width}
        mixed_modal_latent_height: ${model.showo.hq_image_latent_height}
        mixed_modal_latent_width: ${model.showo.hq_image_latent_width}
        min_res: [ 256, 256 ] # minimum image resolution
        random_und_or_gen: 0.0

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 0.0001
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0
        epsilon: 1e-8

lr_scheduler:
    scheduler: "constant_with_warmup"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 0

transport:
    path_type: "Linear"
    prediction: "velocity"
    loss_weight: null
    train_eps: null
    sample_eps: null
    snr_type: "lognorm"
    sampling_method: "euler"
    guidance_scale: 5.0
    num_inference_steps: 50
    atol: 1e-6
    rtol: 1e-3
    reverse: False
    do_shift: True
    time_shifting_factor: 3.0

training:
    gradient_accumulation_steps: 1
    batch_size_t2i: 6
    batch_size_mmu: 2
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10000
    max_train_steps: 40000
    cond_dropout_prob: 0.1
    label_smoothing: 0.0
    max_grad_norm: 1.0
    ntp_coeff: 0.2
    flow_coeff: 1.0
    und_max_t0: 1.0
