wandb:
  entity: null
#  run_id: blvqij6q
  resume: 'auto'
# -2 from pretraining with lr2e-5
experiment:
    project: "showo2-1.5b-stage-2-release"
    name: "showo2-1.5b-stage-2-c"
    output_dir: "showo2-1.5b-stage-2-c"
    max_train_examples_t2i: 100000000 # 100M
    max_train_examples_mmu: 100000000 # 100M
    save_every: 10000000
    eval_every: 2500
    generate_every: 10000000
    log_every: 50
    log_grad_norm_every: 500
    resume_from_checkpoint: 'latest'

model:
    vae_model:
        type: "wan21"
        pretrained_model_path: "/mnt/bn/vgfm2/test_mlx/xavier/code/0923/show-o-next-v5/Wan2.1_VAE.pth"

    showo:
        model_name: "Showo2"
        vision_encoder: "clip"
        load_from_showo: False
        pretrained_model_path: ""
        llm_model_path: "Qwen/Qwen2.5-1.5B-Instruct"
        llm_vocab_size: null # will be updated when setting the tokenizer in other code files
        hidden_size: 1536
        image_latent_dim: 16
        image_latent_height: 27
        image_latent_width: 27
        hq_image_latent_height: 64
        hq_image_latent_width: 64
        mixed_modal_latent_height: 27
        mixed_modal_latent_width: 27
        patch_size: 2
        num_diffusion_layers: 10
        clip_latent_dim: 1152
        add_qk_norm: True
        add_time_embeds: True
        frozen_params: null
        params_not_load: null

    clip:
      pretrained_model_path: "google/siglip-so400m-patch14-384"

    gradient_checkpointing: True

dataset:
    samp_probs: null
    accumulation: 1
    mixed_loader_mode: "concat_max_size_cycle"
    params:
        train_t2i_shards_path_or_url: "path/to/your.jsonl"
        train_mmu_shards_path_or_url: "path/to/your/image/dir"
        annotation_path: "path/to/llavaov-si-img-txt-3.2m.json"
        is_clip_encoder: False
        default_system_prompt: "system\nYou are a helpful assistant.<|im_end|>"
        add_caption_prompt: True
        validation_prompts_file: "prompts/t2i_prompts.txt"
        shuffle_buffer_size: 1000
        num_workers: 6
        pin_memory: True
        persistent_workers: True

    preprocessing:
        max_seq_length: 1024
        max_hq_seq_length: 4352
        max_mixed_modal_seq_length: 5120
        resolution: 432
        hq_resolution: 1024
        mixed_modal_resolution: 432
        num_t2i_image_tokens: 729
        num_mmu_image_tokens: 729
        num_hq_image_tokens: 4096
        num_mixed_modal_tokens: 729
        num_video_tokens: 3645
        latent_height: ${model.showo.image_latent_height}
        latent_width: ${model.showo.image_latent_width}
        hq_latent_height: ${model.showo.hq_image_latent_height}
        hq_latent_width: ${model.showo.hq_image_latent_width}
        mixed_modal_latent_height: ${model.showo.hq_image_latent_height}
        mixed_modal_latent_width: ${model.showo.hq_image_latent_width}
        min_res: [512, 512]
        random_und_or_gen: 0.0

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate_ve: 0.000002
        learning_rate_proj: 0.00001
        learning_rate_showo: 0.00001
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        warmup_steps: null # will be calculated in the training code
        warmup_ratio: 0.03

transport:
    path_type: "Linear"
    prediction: "velocity"
    loss_weight: null
    train_eps: null
    sample_eps: null
    snr_type: "lognorm"
    sampling_method: "euler"
    guidance_scale: 5.0
    num_inference_steps: 50
    atol: 1e-6
    rtol: 1e-3
    reverse: False
    do_shift: True
    time_shifting_factor: 3.0

training:
    gradient_accumulation_steps: 1
    batch_size_t2i: 4
    batch_size_mmu: 4
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 1008
    max_train_steps: null # will be updated in the training code
    cond_dropout_prob: 0.1
    label_smoothing: 0.0
    max_grad_norm: 1.0
    ntp_coeff: 1.0
    flow_coeff: 1.0
    stage: "tuning"
    noise_und_image: True
    und_max_t0: 1.0
