import json
import os
from pathlib import Path
from tqdm import tqdm


def download_textvqa_full(output_dir=None):
    if output_dir is None:
        script_dir = Path(__file__).parent
        output_dir = script_dir / "data" / "textvqa"
    else:
        output_dir = Path(output_dir)
    
    print("=" * 60)
    print("TextVQA Dataset Download (10000 —Å–µ–º–ø–ª–æ–≤)")
    print("=" * 60)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ TextVQA —á–µ—Ä–µ–∑ Hugging Face...")
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ train split...")
        dataset = load_dataset("textvqa", split="train")
        
        max_samples = 10000
        total_samples = len(dataset)
        samples_to_use = min(max_samples, total_samples)
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {samples_to_use} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {total_samples} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_samples})")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        dataset_subset = dataset.select(range(samples_to_use))
        for idx, item in enumerate(tqdm(dataset_subset, desc="Processing", total=samples_to_use)):
            image = item['image']
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            if 'image_id' in item:
                image_key = f"textvqa_{item['image_id']}"
            else:
                image_key = f"textvqa_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            answers = item.get('answers', [])
            if isinstance(answers, list):
                answer_texts = answers[:5]  # –¢–æ–ø-5
                answer = answers[0] if len(answers) > 0 else ''
            else:
                answer = str(answers) if answers else ''
                answer_texts = [answer]
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts,
                "answer": answer
            }
            
            data_list.append(data_item)
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ TextVQA –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_vqav2_full(output_dir=None):
    if output_dir is None:
        script_dir = Path(__file__).parent
        output_dir = script_dir / "data" / "vqav2"
    else:
        output_dir = Path(output_dir)
    
    print("=" * 60)
    print("VQAv2 Dataset Download (10000 —Å–µ–º–ø–ª–æ–≤)")
    print("=" * 60)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ VQAv2 —á–µ—Ä–µ–∑ Hugging Face...")
        dataset = load_dataset("HuggingFaceM4/VQAv2", split="train")
        max_samples = 10000
        total_samples = len(dataset)
        samples_to_use = min(max_samples, total_samples)
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {samples_to_use} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {total_samples} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_samples})")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        
        dataset_subset = dataset.select(range(samples_to_use))
        for idx, item in enumerate(tqdm(dataset_subset, desc="Processing", total=samples_to_use)):
            image = item['image']
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            if 'image_id' in item:
                image_key = f"vqav2_{item['image_id']}"
            else:
                image_key = f"vqav2_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            answers = item.get('answers', [])
            if isinstance(answers, list):
                answer_texts = answers[:5]
                answer = answers[0] if len(answers) > 0 else ''
            else:
                answer = str(answers) if answers else ''
                answer_texts = [answer]
            
            data_item = {
                "image": image_filename,
                "question": item.get('question', ''),
                "answers": answer_texts,
                "answer": answer
            }
            
            data_list.append(data_item)
        
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ VQAv2 –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_docvqa_full(output_dir=None):
    if output_dir is None:
        script_dir = Path(__file__).parent
        output_dir = script_dir / "data" / "docvqa"
    else:
        output_dir = Path(output_dir)
    
    print("=" * 60)
    print("DocVQA Dataset Download (1000 —Å–µ–º–ø–ª–æ–≤)")
    print("=" * 60)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        from datasets import load_dataset
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ DocVQA —á–µ—Ä–µ–∑ Hugging Face...")
        
        dataset_names = [
            "ashraq/docvqa",
            "docvqa",
            "allenai/docvqa",
        ]
        
        dataset = None
        for name in dataset_names:
            try:
                print(f"   –ü—Ä–æ–±—É–µ–º {name}...")
                dataset = load_dataset(name, split="train")
                print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω —á–µ—Ä–µ–∑ {name}")
                break
            except Exception as e:
                print(f"   ‚ùå {name}: {str(e)[:100]}")
                continue
        
        if dataset is None:
            raise ValueError("DocVQA –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ Hugging Face. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å OKVQA –∏–ª–∏ GQA –≤–º–µ—Å—Ç–æ DocVQA.")
        
        max_samples = 1000
        total_samples = len(dataset)
        samples_to_use = min(max_samples, total_samples)
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {samples_to_use} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {total_samples} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_samples})")
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        dataset_subset = dataset.select(range(samples_to_use))
        for idx, item in enumerate(tqdm(dataset_subset, desc="Processing", total=samples_to_use)):
            image = item['image']
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            if 'image_id' in item:
                image_key = f"docvqa_{item['image_id']}"
            else:
                image_key = f"docvqa_{idx:06d}"
            
            if image_key not in seen_images:
                image_filename = f"{image_key}.png"
                image_path = images_dir / image_filename
                image.save(image_path)
                seen_images[image_key] = image_filename
            
            image_filename = seen_images[image_key]
            
            # DocVQA —Ñ–æ—Ä–º–∞—Ç
            question = item.get('question', '')
            answers = item.get('answers', [])
            if isinstance(answers, list) and len(answers) > 0:
                answer = str(answers[0]) if answers[0] else ''
            else:
                answer = str(answers) if answers else ''
            
            data_item = {
                "image": image_filename,
                "question": str(question),
                "answers": [str(a) for a in answers] if isinstance(answers, list) else [str(answers)],
                "answer": str(answer)
            }
            
            data_list.append(data_item)
        
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ DocVQA –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  Hugging Face datasets –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_clevr_full(output_dir=None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç CLEVR –¥–∞—Ç–∞—Å–µ—Ç (10000 —Å–µ–º–ø–ª–æ–≤)
    
    CLEVR —Å–∫–∞—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞–ø—Ä—è–º—É—é —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞: https://cs.stanford.edu/people/jcjohns/clevr/
    """
    if output_dir is None:
        script_dir = Path(__file__).parent
        output_dir = script_dir / "data" / "clevr"
    else:
        output_dir = Path(output_dir)
    
    print("=" * 60)
    print("CLEVR Dataset Download (10000 —Å–µ–º–ø–ª–æ–≤)")
    print("=" * 60)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    images_dir = output_dir / "images"
    images_dir.mkdir(exist_ok=True)
    
    try:
        import requests
        import zipfile
        import tempfile
        import shutil
        
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ CLEVR –Ω–∞–ø—Ä—è–º—É—é...")
        
        # URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è CLEVR (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –¥–∞–Ω–Ω—ã–µ
        clevr_urls = [
            "https://cs.stanford.edu/people/jcjohns/clevr/CLEVR_v1.0.zip",
            "https://dl.fbaipublicfiles.com/clevr/CLEVR_v1.0.zip",
        ]
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        temp_dir = tempfile.mkdtemp()
        zip_path = Path(temp_dir) / "CLEVR_v1.0.zip"
        
        downloaded = False
        for url in clevr_urls:
            try:
                print(f"   –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å —Å: {url}")
                print("   –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è (–¥–∞—Ç–∞—Å–µ—Ç ~20GB)...")
                response = requests.get(url, stream=True, timeout=30)
                response.raise_for_status()
                
                total_size = int(response.headers.get('content-length', 0))
                print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {total_size / (1024**3):.2f} GB")
                
                with open(zip_path, 'wb') as f:
                    for chunk in tqdm(response.iter_content(chunk_size=8192), 
                                     total=total_size // 8192, 
                                     desc="   –°–∫–∞—á–∏–≤–∞–Ω–∏–µ", 
                                     unit="KB"):
                        f.write(chunk)
                
                print("   ‚úÖ –§–∞–π–ª —Å–∫–∞—á–∞–Ω")
                downloaded = True
                break
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {str(e)[:100]}")
                continue
        
        if not downloaded:
            print("\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å CLEVR –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.")
            print("   –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–∞—á–∞—Ç—å –≤—Ä—É—á–Ω—É—é:")
            print("   1. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ https://cs.stanford.edu/people/jcjohns/clevr/")
            print("   2. –°–∫–∞—á–∞–π—Ç–µ CLEVR_v1.0.zip")
            print("   3. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return False
        
        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º ZIP
        print("   –†–∞—Å–ø–∞–∫–æ–≤–∫–∞ –∞—Ä—Ö–∏–≤–∞...")
        extract_dir = Path(temp_dir) / "clevr_extracted"
        extract_dir.mkdir(exist_ok=True)
        
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        questions_file = None
        images_source_dir = None
        
        for root, dirs, files in os.walk(extract_dir):
            for file in files:
                if 'questions' in file and file.endswith('.json'):
                    questions_file = Path(root) / file
                if 'train' in dirs:
                    images_source_dir = Path(root) / 'images' / 'train'
                    break
            if questions_file and images_source_dir:
                break
        
        if not questions_file or not images_source_dir:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            questions_file = extract_dir / "CLEVR_v1.0" / "questions" / "CLEVR_train_questions.json"
            images_source_dir = extract_dir / "CLEVR_v1.0" / "images" / "train"
        
        if not questions_file.exists() or not images_source_dir.exists():
            print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã:")
            print(f"      Questions: {questions_file}")
            print(f"      Images: {images_source_dir}")
            return False
        
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã:")
        print(f"      Questions: {questions_file}")
        print(f"      Images: {images_source_dir}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã
        with open(questions_file, 'r') as f:
            questions_data = json.load(f)
        
        questions = questions_data.get('questions', [])
        print(f"   –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions)}")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10000 —Å–µ–º–ø–ª–æ–≤
        max_samples = 10000
        samples_to_use = min(max_samples, len(questions))
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {samples_to_use} –ø—Ä–∏–º–µ—Ä–æ–≤ (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: {max_samples})")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_files = list(images_source_dir.glob("*.png"))
        image_dict = {img.stem: img for img in image_files}
        
        data_list = []
        print("üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        seen_images = {}
        
        for idx, item in enumerate(tqdm(questions[:samples_to_use], desc="Processing", total=samples_to_use)):
            image_filename_hf = item.get('image_filename', '')
            image_id = item.get('image_index', idx)
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_path = None
            if image_filename_hf:
                image_path = images_source_dir / image_filename_hf
            elif str(image_id) in image_dict:
                image_path = image_dict[str(image_id)]
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É
                for img_file in image_files:
                    if str(image_id) in img_file.stem or image_filename_hf in img_file.name:
                        image_path = img_file
                        break
            
            if not image_path or not image_path.exists():
                continue
            
            # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_key = f"clevr_{image_id}"
            if image_key not in seen_images:
                dest_image_path = images_dir / f"{image_key}.png"
                shutil.copy2(image_path, dest_image_path)
                seen_images[image_key] = f"{image_key}.png"
            
            image_filename = seen_images[image_key]
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            question = item.get('question', '')
            answer = item.get('answer', '')
            
            data_item = {
                "image": image_filename,
                "question": str(question),
                "answers": [str(answer)] if answer else [],
                "answer": str(answer)
            }
            
            data_list.append(data_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        train_json_path = output_dir / "train.json"
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(data_list, f, indent=2, ensure_ascii=False)
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        shutil.rmtree(temp_dir)
        
        print(f"‚úÖ CLEVR –∑–∞–≥—Ä—É–∂–µ–Ω: {len(data_list)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(seen_images)}")
        print(f"   JSON: {train_json_path}")
        print(f"   Images: {images_dir}")
        return True
        
    except ImportError:
        print("‚ö†Ô∏è  –ù—É–∂–Ω—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: requests. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ MoE (–ø–æ 10000 —Å–µ–º–ø–ª–æ–≤ –∫–∞–∂–¥—ã–π)')
    parser.add_argument('--textvqa', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å TextVQA (10000 —Å–µ–º–ø–ª–æ–≤)')
    parser.add_argument('--vqav2', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å VQAv2 (10000 —Å–µ–º–ø–ª–æ–≤)')
    parser.add_argument('--docvqa', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å DocVQA (10000 —Å–µ–º–ø–ª–æ–≤) - –¥–æ–∫—É–º–µ–Ω—Ç—ã')
    parser.add_argument('--clevr', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å CLEVR (10000 —Å–µ–º–ø–ª–æ–≤) - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ 3D —Å—Ü–µ–Ω—ã')
    parser.add_argument('--all', action='store_true', help='–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (–ø–æ 10000 —Å–µ–º–ø–ª–æ–≤ –∫–∞–∂–¥—ã–π)')
    parser.add_argument('--output-dir', type=str, default=None,
                       help='–ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: moe_experiments/data –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–∫—Ä–∏–ø—Ç–∞)')
    
    args = parser.parse_args()
    
    success_count = 0
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å
    if args.output_dir:
        base_output_dir = Path(args.output_dir)
    else:
        script_dir = Path(__file__).parent
        base_output_dir = script_dir / "data"
    
    if args.all or args.textvqa:
        if args.output_dir:
            textvqa_output = base_output_dir / "textvqa"
        else:
            textvqa_output = None
        if download_textvqa_full(textvqa_output):
            success_count += 1
    
    if args.all or args.vqav2:
        if args.output_dir:
            vqav2_output = base_output_dir / "vqav2"
        else:
            vqav2_output = None
        if download_vqav2_full(vqav2_output):
            success_count += 1
    
    if args.all or args.docvqa:
        if args.output_dir:
            docvqa_output = base_output_dir / "docvqa"
        else:
            docvqa_output = None
        if download_docvqa_full(docvqa_output):
            success_count += 1
    
    if args.all or args.clevr:
        if args.output_dir:
            clevr_output = base_output_dir / "clevr"
        else:
            clevr_output = None
        if download_clevr_full(clevr_output):
            success_count += 1
    
    if success_count == 0:
        print("\n‚ö†Ô∏è  –ù–µ —É–∫–∞–∑–∞–Ω—ã –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.")
        print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --textvqa, --vqav2, --docvqa, --clevr –∏–ª–∏ --all")
        print("\n–ü—Ä–∏–º–µ—Ä—ã:")
        print("  python download_full_datasets.py --docvqa")
        print("  python download_full_datasets.py --clevr")
        print("  python download_full_datasets.py --docvqa --clevr  # –î–ª—è –¥–≤—É—Ö —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤")
    else:
        print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {success_count}")


if __name__ == "__main__":
    main()

